{"cells":[{"metadata":{},"cell_type":"markdown","source":" #             "},{"metadata":{},"cell_type":"markdown","source":"Team: \n   1. Joe Lynch joetlynch <jlynch@wonderbench.com>\n   2. Pallav Sharda pallavsharda <pallavsharda@gmail.com>\n   3. McGreevy, Robert J <robert.mcgreevy@abbott.com>\n   4. George Tolkachev geotolky <georgeto@seas.upenn.edu>,\n   5. Aditya Gudal IsYourDataFit? <adityagudal2020@u.northwestern.edu>"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement\n\n### COVID-19 is an emerging, rapidly evolving situation. Health care professionals and the public at large are inundated daily with new findings, observations, and studies. A simple “COVID-19” PubMed search on April 15, 2020 quickly frames the issue."},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"/kaggle/input/image-covid/sars_imp.PNG\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Medical professionals looking to gain perspective from the trusted PubMed repository of studies quickly find that 99.8% of PubMed studies were published in 2020 and the number of daily published studies has rapidly increased beyond an average of 150 studies per day during the first half of April, and the rate is increasing. 262 studies were published on April 14 alone. In the absence of a well defined clinical trial and/or a simple means for gauging the clinical utility of a broad range of studies, how are medical professionals to sort through, understand and enhance their care and treatment and intervention plans from this quickly growing heap of published data?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/coronavirus-image/SARS-CoV-2_without_background.png\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Goal\n\n### *Our team brought together by Wonderbench and comprised of data scientists, NLP students, and MedTech professionals aims to use Artificial Intelligence (AI) and Natural Language Processing (NLP) to understand and characterize the clinical utility of therapeutic studies so that the clinical utility and impact of newly published studies can be immediately assessed. In collaboration with the broader community, we look to develop an AI-driven guidance tool that would serve to support medical professionals in quickly highlighting which of the newest studies may be most deserving of their immediate time, attention and consideration toward the goal of rapidly improving the intervention and treatment plan for COVID-19 patients.*\n"},{"metadata":{},"cell_type":"markdown","source":"## Approach\n1. Create a corpus of PubMed and Elsevier Studies \n    * We have focused only on  studies published the last 6 months( November 2019 onwards), accessible via the CORD-19 dataset.\n    * Extracted all json files of articles and created a unified dataset/table.\n    * Created trivial features such as word count, character count, average word length, number of stopwords, number of special characters, number of numerics.\n    * Cleaned the body text of every article(Removed stop words, lowercased words, removed punctuations and special characters).\n\n2. Characterize studies and derive insights from them based on the following approaches:\n    * Approach 1: Analyze the keywords assigned to each article by Pubmed (available via pubmed).\n    * Approach 2: Analyzing based on MesH (Medical Subject Headings). MesH is a controlled semantic vocabulary used by NLM/Pubmed. We plan to use the ‘MeSH on Demand’ tool to automatically identify concepts and characterize the articles. For example, COVID-19 is already a concept in MeSH and we can intersect that with the Therapeutics concept class.\n    * Approach 3: We can also apply other entity extraction tools like Scispacy, AWS Comprehend Medical, Google Cloud NL API.\n    * Present the findings and insights to domain experts (Infectious Disease clinicians, researchers) and get feedback on the relative value of the output from the three approaches (and the combination of one/more approaches). Based on that, refine the model and re-run the analysis.\n    \n3. We also want to work on creating a database and querying tool that will help physicians and health workers to find articles related to medical care by producing meta tags for every article, so it saves time for health workers. The database would contain articles from pubmed, semantic scholar, kaggle database and elsevier.\n\n\n#### What has been done yet?\n* We show you that we have created a topic model using the latent dirichlet allocation algorithm that fetches us 30 topics our training corpus is created using the term frequency inverse document frequency algorithm and bag of words model that is trained on 6000 articles. \n* We also show a word cloud using the common words found in these article masked by the coronvirus image. \n* We also have used a word2vec model that shows the similarity of particular words in these articles to words such as treatment, therapeutic, medical, interventions etc. \n* This is just our initial exploration as we started a week back. We assure you we will bring something concrete soon. Thanks\n\n#### Based on these we will be creating meta tags and also train our model using scispacy a library having model parsers that are trained on large texts of scientific, biomedical research.\n\n\n\n#### *Once a useful output is created, we plan to create a continually-updating service that can ingest, extract and analyze content related to Covid-19 related therapeutic research. That service can be utilized by clinicians/researchers in a web or mobile application.*\n \n\n\n\nWe welcome feedback so that we can continue to improve this project.\n"},{"metadata":{},"cell_type":"markdown","source":"### Load Libraries important libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\nimport scipy\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for all the files required"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls /kaggle/input/CORD-19-research-challenge/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Explore the meta data given"},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We look at how many articles published were from different sources? "},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df['source_x'].value_counts(normalize=True).plot(kind='bar',figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)How many missing values are present in our dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.apply(lambda x:sum(x.isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We just want to use articles November 2019 onwards"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df['year']=pd.to_datetime(meta_df.publish_time).dt.year\nmeta_df['month']=pd.to_datetime(meta_df.publish_time).dt.month\n\nmeta_19_20=meta_df.loc[(meta_df.year.isin([2019,2020]))]\n\ndef get_data(dataframe):\n    \n    if dataframe['year']==2019:\n        dataframe=dataframe.loc[dataframe['month'].isin([11,12])]\n    return dataframe\n\nmeta_19_20=meta_19_20.sort_values(by=['year','month'])\nmeta_19=meta_19_20.iloc[np.hstack(np.argwhere((meta_19_20['year']==2019) & meta_19_20['month'].isin([11,12]))),:]\nmeta_20=meta_19_20.loc[meta_19_20['year']==2020]\n\nmeta_df=pd.concat([meta_19,meta_20],ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many missing values are there in our fields now?"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.apply(lambda x:sum(x.isna()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many articles are there originally?"},{"metadata":{},"cell_type":"markdown","source":"Modified some part of the file loading code from: https://www.kaggle.com/maksimeren/covid-19-literature-clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            try:\n                if content['abstract']:\n                \n                    for entry in content['abstract']:\n                        self.abstract.append(entry['text'])\n            except:\n                self.abstract.append('NA')\n                \n            \n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[10000])\nprint(first_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function parses all the relevant articles and extracts the abstracts and text required."},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in tqdm.tqdm(enumerate(all_json)):\n    \n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 'NA': \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's clean our text and create unigrams,bigrams and trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.corpus import wordnet as wn\nimport os\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models.word2vec import LineSentence\nstop = stopwords.words('english')\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n    \n\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)\n\ndef text_cleaner(Series):\n    \n    Series=Series.dropna()\n    \n    Series=Series.apply(lambda x: \" \".join(word.lower() for word in str(x).split()))\n    Series=Series.apply(lambda x: \" \".join(word for word in str(x).split() if word not in stop))\n    Series=Series.str.replace('[^\\w\\s]','')\n    Series=Series.apply(lambda x: \" \".join(get_lemma(word) for word in str(x).split()))\n    Series=Series.apply(lambda x: \" \".join(word for word in str(x).split() if len(word)>3))\n    Series=Series.apply(lambda x:\" \".join(word for word in str(x).split() if word.isalpha()))\n    \n    months=['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n    \n    Series=Series.apply(lambda x:\" \".join(word for word in str(x).split() if word not in months))\n    \n    \n    unigrams = Series.apply(word_tokenize)\n    bigram_phrases = Phrases(unigrams)\n        \n    bigram_phrases = Phraser(bigram_phrases)\n    \n    sentences_bigrams_filepath = os.path.join(os.getcwd(), str(Series.name)+'_sentence_bigram_phrases_all.txt')\n    \n    with open(sentences_bigrams_filepath, 'w') as f:\n        \n        for sentence_unigrams in tqdm.tqdm(unigrams):\n            \n            sentence_bigrams = ' '.join(bigram_phrases[sentence_unigrams])\n            \n            f.write(sentence_bigrams + '\\n')\n    sentences_bigrams = LineSentence(sentences_bigrams_filepath)\n    \n    for sentence_bigrams in tqdm.tqdm(it.islice(sentences_bigrams, 60, 70)):\n        print(' '.join(sentence_bigrams))\n        print('')\n    \n    trigram_phrases = Phrases(sentences_bigrams)\n    \n    # Turn the finished Phrases model into a \"Phraser\" object,\n    # which is optimized for speed and memory use\n    trigram_phrases = Phraser(trigram_phrases)\n    \n    sentences_trigrams_filepath = os.path.join(os.getcwd(),str(Series.name)+ '_sentence_trigram_phrases_all.txt')\n    with open(sentences_trigrams_filepath, 'w') as f:\n        \n        for sentence_bigrams in tqdm.tqdm(sentences_bigrams):\n            \n            sentence_trigrams = ' '.join(trigram_phrases[sentence_bigrams])\n            \n            f.write(sentence_trigrams + '\\n')\n            \n    sentences_trigrams = LineSentence(sentences_trigrams_filepath)\n    \n    for sentence_trigrams in tqdm.tqdm(it.islice(sentences_trigrams, 60, 70)):\n        print(' '.join(sentence_trigrams))\n        print('')\n        \n    return sentences_trigrams_filepath,Series\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now create two models using LDA topic modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom collections import Counter\n\n\nimport gensim\nfrom gensim.corpora import Dictionary, MmCorpus\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim import corpora, models\n\nimport itertools as it\n\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport warnings\nimport pickle\n\nfrom pprint import pprint\n\n\n\ndef bow_generator(filepath,dictionary_trigrams):\n    \"\"\"\n    generator function to read reviews from a file\n    and yield a bag-of-words representation\n    \"\"\"\n    \n    for review in LineSentence(filepath):\n        yield dictionary_trigrams.doc2bow(review)\n\ndef topic_modeling(Series_path):\n    series_list=LineSentence(Series_path)\n\n    lists=[]\n    for item in series_list:\n        lists.append(item)\n        \n    tokens=word_tokenize(' '.join(word for item in lists for word in item))\n    \n    print(Counter(tokens).most_common(50))\n    \n    articles_trigrams = LineSentence(Series_path)\n\n    # learn the dictionary by iterating over all of the reviews\n    dictionary_trigrams = Dictionary(articles_trigrams)\n    \n    bow_corpus_filepath = os.path.join(os.getcwd(), 'bow_trigrams_corpus_all.mm')\n    \n    MmCorpus.serialize(\n        bow_corpus_filepath,\n        bow_generator(Series_path,dictionary_trigrams)\n        )\n    \n    trigram_bow_corpus = MmCorpus(bow_corpus_filepath)\n    \n    \n    tfidf = models.TfidfModel(trigram_bow_corpus)\n    corpus_tfidf = tfidf[trigram_bow_corpus]\n    \n        \n    lda_bow_model = gensim.models.LdaMulticore(trigram_bow_corpus, num_topics=30, id2word=dictionary_trigrams, passes=2, workers=2)\n    \n    print('LDA BoW MODEL')\n    for idx, topic in lda_bow_model.print_topics(-1):\n        print('Topic: {} \\nWords: {}'.format(idx, topic))\n        \n    lda_tfidf_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=30, id2word=dictionary_trigrams, passes=2, workers=2)\n    \n    print('LDA TFIDF MODEL')\n    for idx, topic in lda_tfidf_model.print_topics(-1):\n        print('Topic: {} \\nWords: {}'.format(idx, topic))\n        \n    return lda_bow_model,lda_tfidf_model,dictionary_trigrams,trigram_bow_corpus,corpus_tfidf\n    \n    \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleaning the text and creating the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"abstract_path,clean_abstract=text_cleaner(df_covid['body_text'])\nlda_bow_model,lda_tfidf_model,dictionary_trigrams,trigram_bow_corpus,corpus_tfidf=topic_modeling(abstract_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SCISPACY BASED TOPICS"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/topic-eg/topic_3.PNG\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like it's on COVID-19 around countries and their risks"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/topic-eg/topic_4.PNG\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This topic is based on a certain kind of rna sequence related to covid"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/topic-eg/topic_5.PNG\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This topic is based on covid treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/topics-used/topic_1.PNG\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again protein and gene sequence also vaccinations"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(\"/kaggle/input/topics-used/topic_2.PNG\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"COVID-19 patient cases "},{"metadata":{},"cell_type":"markdown","source":"Explore the topics"},{"metadata":{},"cell_type":"markdown","source":"TOPIC MODELED BY our general model "},{"metadata":{"trusted":true},"cell_type":"code","source":"def explore_topic(model,topic_number, topn=25):\n    \"\"\"\n    accept a user-supplied topic number and\n    print out a formatted list of the top terms\n    \"\"\"\n        \n    print(f'{\"term\":20} {\"frequency\"}' + '\\n')\n\n    for term, frequency in model.show_topic(topic_number, topn=40):\n        print(f'{term:20} {round(frequency, 3):.3f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BAG OF WORDS VERSUS TFIDF MODEL - 40 topics look at the 10th one"},{"metadata":{"trusted":true},"cell_type":"code","source":"explore_topic(lda_bow_model,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explore_topic(lda_tfidf_model,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting to look at certain common words in these articles using wordclouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nvirus_mask = np.array(Image.open(\"/kaggle/input/coronavirus-image/SARS-CoV-2_without_background.png\"))\nstopwords = set(STOPWORDS)\ntext=\" \".join(word for abstract in clean_abstract for word in abstract.split())\nwc = WordCloud(background_color=\"white\", max_words=1000, mask=virus_mask,\n               stopwords=stopwords, max_font_size=50, random_state=42,contour_width=4, contour_color='firebrick')\n# generate word cloud\nwc.generate(text)\n\n# create coloring from image\nimage_colors = ImageColorGenerator(virus_mask)\n\n# show\nfig, axes = plt.subplots(1,2,figsize=(50,50))\n# recolor wordcloud and show\n# we could also give color_func=image_colors directly in the constructor\naxes[0].imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\naxes[1].imshow(virus_mask, cmap=plt.cm.gray, interpolation=\"bilinear\")\nfor ax in axes:\n    ax.set_axis_off()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We generate word embeddings for every text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n\nsentences_trigrams = LineSentence(abstract_path)\nword2vec_filepath = os.path.join(os.getcwd(), 'word2vec_model_all')\n\ncorona2vec = Word2Vec(\n        sentences_trigrams,\n        size=100,\n        window=5,\n        min_count=50,\n        sg=1,\n        workers=7,\n        iter=20\n        )\n\ncorona2vec.init_sims()\n\nprint(f'{corona2vec.epochs} training epochs so far.')\n\nprint(f'{len(corona2vec.wv.vocab):,} terms in the corona2vec vocabulary.')\n\n# build a list of the terms, integer indices,\n# and term counts from the food2vec model vocabulary\nordered_vocab = [\n    (term, voc.index, voc.count)\n    for term, voc in corona2vec.wv.vocab.items()\n    ]\n\n# sort by the term counts, so the most common terms appear first\nordered_vocab = sorted(ordered_vocab, key=lambda term_tuple: -term_tuple[2])\n\n# unzip the terms, integer indices, and counts into separate lists\nordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n\n# create a DataFrame with the food2vec vectors as data,\n# and the terms as row labels\nword_vectors = pd.DataFrame(\n    corona2vec.wv.vectors_norm[term_indices, :],\n    index=ordered_terms\n    )\n\nword_vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TSNE reduces the word embedding dimensionality space and is used to look at similar words clustered together"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne_input = (\n    word_vectors\n    .head(5000)\n    )\n\ntsne_input.head()\n\ntsne_filepath = os.path.join(os.getcwd(), 'tsne_model')\n\ntsne_vectors_filepath = os.path.join(os.getcwd(), 'tsne_vectors.npy')\n\ntsne = TSNE()\ntsne_vectors = tsne.fit_transform(tsne_input.values)\n    \nwith open(tsne_filepath, 'wb') as f:\n    pickle.dump(tsne, f)\n    \n    \ntsne_vectors = pd.DataFrame(\n    tsne_vectors,\n    index=pd.Index(tsne_input.index),\n    columns=['x_coord', 'y_coord']\n    )\n\ntsne_vectors['word'] = tsne_vectors.index\ntsne_vectors.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EVERYTHING BELOW SHOWS THE SIMILARITY OF SOME WORDS WHICH WOULD BE USED IN OUT META TAG AND WEB APP**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, value\n\noutput_notebook()\n\n# add our DataFrame as a ColumnDataSource for Bokeh\nplot_data = ColumnDataSource(tsne_vectors)\n\n# create the plot and configure the\n# title, dimensions, and tools\ntsne_plot = figure(\n    title='t-SNE Word Embeddings',\n    plot_width=800,\n    plot_height=800,\n    tools=(\n        'pan, wheel_zoom, box_zoom,'\n        'box_select, reset'\n        ),\n    active_scroll='wheel_zoom'\n    )\n\n# add a hover tool to display words on roll-over\ntsne_plot.add_tools(\n    HoverTool(tooltips = '@word')\n    )\n\n# draw the words as circles on the plot\ntsne_plot.circle(\n    'x_coord',\n    'y_coord',\n    source=plot_data,\n    color='blue',\n    line_alpha=0.2,\n    fill_alpha=0.1,\n    size=10,\n    hover_line_color='black'\n    )\n\n# configure visual elements of the plotc\ntsne_plot.title.text_font_size = value('16pt')\ntsne_plot.xaxis.visible = False\ntsne_plot.yaxis.visible = False\ntsne_plot.grid.grid_line_color = None\ntsne_plot.outline_line_color = None\n\n# engage!\nshow(tsne_plot);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HOVER OVER TO HAVE A LOOK AT RELATED TERMS"},{"metadata":{},"cell_type":"markdown","source":"Look at the related terms."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import cm\ndef get_related_terms(token, topn=20):\n    \"\"\"\n    look up the topn most similar terms to token\n    and print them as a formatted list\n    \"\"\"\n    \n    words,similarities=[],[]\n    \n    for word, similarity in corona2vec.wv.most_similar(positive=[token], topn=topn):\n\n        print(f'{word:20} {round(similarity, 3)}')\n        \n        words.append(word)\n        \n        similarities.append(similarity)\n        \n    plt.style.use('ggplot')\n    \n    \n    pd.DataFrame(data=similarities,index=words).sort_values(by=0,ascending=True).plot(kind='barh',cmap=cm.get_cmap('Spectral'),figsize=(10,10))\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows what terms are similar to our query"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('intervention')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('medical_care')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('vaccine')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('infection')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('steroid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('virus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('protein')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('research')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('public_health_intervention')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('treatment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('medicine')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('care')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('symptom')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('disease')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('literature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('covid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('sars')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('cure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_related_terms('medical')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VISUALIZATION ON 2 COORDINATES: SHOWING ALL 30 TOPICS BASED ON THE TFIDF LDA MODEL\n*  *Note: Cluster shows how topics are related*"},{"metadata":{"trusted":true},"cell_type":"code","source":"LDAvis_prepared = pyLDAvis.gensim.prepare(\n        lda_tfidf_model,\n        trigram_bow_corpus,\n        dictionary_trigrams\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.display(LDAvis_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### OUR CONCLUSION TO THIS BRIEF ANALYSIS IS THAT : \n\n#### Using just a few articles out of the 44k articles on kaggle we can see some way of keywords and endpoints that are of interest to the various task. Here we strive to focus on medical care but we will try to create a tool that will be accessible to all the healthworkers risking their lives and working so hard. This is our first stab at the dataset and we look forward to commit and work harder to help our community."},{"metadata":{},"cell_type":"markdown","source":"Thank you judges and we promise we will have something vital and concrete for the next round."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}